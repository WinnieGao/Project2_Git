---
title: "Proj 2"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

* Import data
```{r}
image1 = read.delim(file = "image_data/image1.txt", header = FALSE, sep = "")
colnames(image1) = c("y_coord", "x_coord", "expert_label", "NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")
image2 = read.delim(file = "image_data/image2.txt", header = FALSE, sep = "")
colnames(image2) = c("y_coord", "x_coord", "expert_label", "NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")
image3 = read.delim(file = "image_data/image3.txt", header = FALSE, sep = "")
colnames(image3) = c("y_coord", "x_coord", "expert_label", "NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")
all = rbind(image1, image2, image3)
```

## Problem 1
* 1(b)
```{r}
nrow(image1)
sum(image1$expert_label == 1) / nrow(image1)
sum(image1$expert_label == -1) / nrow(image1)
sum(image1$expert_label == 0) / nrow(image1)
```
For image1, there are 115229 data points. We calculate % of pixels for the different classes: 17.77% classified as cloud, 43.78% as not cloud and 38.46% as unlabeled.
```{r}
nrow(image2)
sum(image2$expert_label == 1) / nrow(image2)
sum(image2$expert_label == -1) / nrow(image2)
sum(image2$expert_label == 0) / nrow(image2)
```
For image2, there are 115110 data points. We calculate % of pixels for the different classes: 34.11% classified as cloud, 37.25% as not cloud and 28.64% as unlabeled.
```{r}
nrow(image3)
sum(image3$expert_label == 1) / nrow(image3)
sum(image3$expert_label == -1) / nrow(image3)
sum(image3$expert_label == 0) / nrow(image3)
```
For image3, there are 115217 data points. We calculate % of pixels for the different classes: 18.44% classified as cloud, 29.29% as not cloud and 52.27% as unlabeled.
```{r}
nrow(all)
sum(all$expert_label == 1) / nrow(all)
sum(all$expert_label == -1) / nrow(all)
sum(all$expert_label == 0) / nrow(all)
```
After we combined three image data sets, there are 345556 data points. We calculate % of pixels for the different classes: 23.43% classified as cloud, 36.78% as not cloud and 39.79% as unlabeled.

```{r}
library(ggplot2)
label = image1$expert_label
label[label == 1] = "cloud"
label[label == -1] = "not cloud"
label[label == 0] = "unlabeled"
image1_plot = data.frame(x = image1$x_coord, y = image1$y_coord, label = label)
ggplot(data = image1_plot,aes(x = x, y = -y, color = label)) + geom_point() + scale_color_manual(values=c("white", "grey", "black"))
```

```{r}
label = image2$expert_label
label[label == 1] = "cloud"
label[label == -1] = "not cloud"
label[label == 0] = "unlabeled"
image2_plot = data.frame(x = image2$x_coord, y = image2$y_coord, label = label)
ggplot(data = image2_plot,aes(x = x, y = -y, color = label)) + geom_point() + scale_color_manual(values=c("white", "grey", "black"))
```
```{r}
label = image3$expert_label
label[label == 1] = "cloud"
label[label == -1] = "not cloud"
label[label == 0] = "unlabeled"
image3_plot = data.frame(x = image3$x_coord, y = image3$y_coord, label = label)
ggplot(data = image3_plot,aes(x = x, y = -y, color = label)) + geom_point() + scale_color_manual(values=c("white", "grey", "black"))
```
From the three plots, we can see the pattern that the pixels with same labels are connected to each other, and unmarked pixels stay around those marked as clouds. This pattern contradicts to our assumption that the samples are i.i.d., because the pixels adjcent to pixels marked as clouds are more likely to be marked as clouds.

* (c)
```{r}
# pairwise relationship between features for image1
features = data.frame(NDAI = image1$NDAI, SD = image1$SD, CORR = image1$CORR, DF = image1$DF, CF = image1$CF, BF = image1$BF, AF = image1$AF, AN = image1$AN)
pairs(features, gap = 0, pch = ".")
```
```{r}
# pairwise relationship between features for image3
features = data.frame(NDAI = image2$NDAI, SD = image2$SD, CORR = image2$CORR, DF = image2$DF, CF = image2$CF, BF = image2$BF, AF = image2$AF, AN = image2$AN)
pairs(features, gap = 0, pch = ".")
```
```{r}
# pairwise relationship between features for image3
features = data.frame(NDAI = image3$NDAI, SD = image3$SD, CORR = image3$CORR, DF = image3$DF, CF = image3$CF, BF = image3$BF, AF = image3$AF, AN = image3$AN)
pairs(features, gap = 0, pch = ".")
```
The three pairwise scatterplots all show that DF, CF, BF, AF and AN shows a linear relationship between each other, especially for AF and AN, AF and BF. However, we observe that data points in the third pairwise scatterplot spread more widely than in the first two pairwise sctterplots. The differences between three graphs can also show that the (linear) relationships decrease over time.

After checking pairwise relationship plots, we study the relationship between each independent featues and their expert labels for each image and all data combined.
```{r}
#relationship between expert label and NDAI image1
expert_NDAI = data.frame(label = as.character(image1$expert_label), CORR = image1$CORR)
expert_NDAI = expert_NDAI[which(expert_NDAI$label != "0"), ]
ggplot(data = expert_NDAI, aes(x = label, y = CORR, group = label, fill = label)) + geom_boxplot() + scale_fill_discrete(name = "Label", labels = c("not cloud", "cloud")) + ggtitle("Label vs. CORR image1")
```

```{r}
#relationship between expert label and NDAI image2
expert_NDAI = data.frame(label =  as.character(image2$expert_label), CORR = image2$CORR)
expert_NDAI = expert_NDAI[which(expert_NDAI$label != "0"), ]
ggplot(data = expert_NDAI, aes(x = label, y = CORR, group = label, fill = label)) + geom_boxplot() + scale_fill_discrete(name = "Label", labels = c("not cloud", "cloud")) + ggtitle("Label vs. CORR image2")
```
```{r}
#relationship between expert label and NDAI image2
expert_NDAI = data.frame(label =  as.character(image3$expert_label), CORR = image3$CORR)
expert_NDAI = expert_NDAI[which(expert_NDAI$label != "0"), ]
ggplot(data = expert_NDAI, aes(x = label, y = CORR, group = label, fill = label)) + geom_boxplot() + scale_fill_discrete(name = "Label", labels = c("not cloud", "cloud")) + ggtitle("Label vs. CORR image3")
```
```{r}
#relationship between expert label and NDAI image2
expert_NDAI = data.frame(label =  as.character(all$expert_label), CORR = all$CORR)
expert_NDAI = expert_NDAI[which(expert_NDAI$label != "0"), ]
ggplot(data = expert_NDAI, aes(x = label, y = CORR, group = label, fill = label)) + geom_boxplot() + scale_fill_discrete(name = "Label", labels = c("not cloud", "cloud")) + ggtitle("Label vs. CORR All Data")
```
The boxplots for CORR based on different labels show that CORR is on average higher for pixels marked as cloud than as not cloud. CORR also has a higher variance for those marked as cloud.

This conclusion contradicts to the statement in the paper that high correlations over cloud-free areas or low clouds are expected. This is because high correlations for clouds also occur under rare circumstances. More importantly, recklessly declare clear for high CORR pixels and cloudy for low CORR pixels will produce errors due to smoothness of surface terrain and the difference of attitudes of clouds.

Therefore, we should also involve the feature SD to identify surfaces into our investigation. We first plot SD vs. Label independently for each image and all data.
```{r}
expert_SD = data.frame(label = as.character(image1$expert_label), SD = image1$SD)
expert_SD = expert_SD[which(expert_SD$label != "0"), ]
ggplot(data = expert_SD, aes(x = label, y = SD, group = label, fill = label)) + geom_boxplot() + scale_fill_discrete(name = "Label", labels = c("not cloud", "cloud")) + ggtitle("Label vs. SD image1")
```
```{r}
expert_SD = data.frame(label = as.character(image2$expert_label), SD = image2$SD)
expert_SD = expert_SD[which(expert_SD$label != "0"), ]
ggplot(data = expert_SD, aes(x = label, y = SD, group = label, fill = label)) + geom_boxplot() + scale_fill_discrete(name = "Label", labels = c("not cloud", "cloud")) + ggtitle("Label vs. SD image2")
```
```{r}
expert_SD = data.frame(label = as.character(image3$expert_label), SD = image3$SD)
expert_SD = expert_SD[which(expert_SD$label != "0"), ]
ggplot(data = expert_SD, aes(x = label, y = SD, group = label, fill = label)) + geom_boxplot() + scale_fill_discrete(name = "Label", labels = c("not cloud", "cloud")) + ggtitle("Label vs. SD image3")
```
```{r}
expert_SD = data.frame(label = as.character(all$expert_label), SD = all$SD)
expert_SD = expert_SD[which(expert_SD$label != "0"), ]
ggplot(data = expert_SD, aes(x = label, y = SD, group = label, fill = label)) + geom_boxplot() + scale_fill_discrete(name = "Label", labels = c("not cloud", "cloud")) + ggtitle("Label vs. SD All Data")
```
From the boxplots for SD based on different labels, we can clearly see that the SD is higher for cloud pixels than those cloud-free ones. But the cloud-free pixels spread more widely. It can be explained that SD are usually small for radiation emanating from smooth surfaces.

Finally, the thrid feature NDAI relates to the differences for isoreopic level of surface-leaving radiation between low-attitude clouds and snow-coved surfaces.
```{r}
expert_NDAI = data.frame(label = as.character(image1$expert_label), NDAI = image1$NDAI)
expert_NDAI = expert_NDAI[which(expert_NDAI$label != "0"), ]
ggplot(data = expert_NDAI, aes(x = label, y = NDAI, group = label, fill = label)) + geom_boxplot() + scale_fill_discrete(name = "Label", labels = c("not cloud", "cloud")) + ggtitle("Label vs. NDAI image1")
```
```{r}
expert_NDAI = data.frame(label = as.character(image2$expert_label), NDAI = image2$NDAI)
expert_NDAI = expert_NDAI[which(expert_NDAI$label != "0"), ]
ggplot(data = expert_NDAI, aes(x = label, y = NDAI, group = label, fill = label)) + geom_boxplot() + scale_fill_discrete(name = "Label", labels = c("not cloud", "cloud")) + ggtitle("Label vs. NDAI image2")
```
```{r}
expert_NDAI = data.frame(label = as.character(image3$expert_label), NDAI = image3$NDAI)
expert_NDAI = expert_NDAI[which(expert_NDAI$label != "0"), ]
ggplot(data = expert_NDAI, aes(x = label, y = NDAI, group = label, fill = label)) + geom_boxplot() + scale_fill_discrete(name = "Label", labels = c("not cloud", "cloud")) + ggtitle("Label vs. NDAI image3")
```
```{r}
expert_NDAI = data.frame(label = as.character(all$expert_label), NDAI = all$NDAI)
expert_NDAI = expert_NDAI[which(expert_NDAI$label != "0"), ]
ggplot(data = expert_NDAI, aes(x = label, y = NDAI, group = label, fill = label)) + geom_boxplot() + scale_fill_discrete(name = "Label", labels = c("not cloud", "cloud")) + ggtitle("Label vs. NDAI All Data")
```

From the boxplots for NDAI based on different labels, we can clearly see that the NDAI is higher for cloud pixels than those cloud-free ones. The distrubition for cloud-free pixels are left skewed, while the distribution for pixels marked as cloud are roughly symmetric. The distribution for three different image data are roughly the same as well.

## Problem 2
* 2(a)
Even though the three image data set represent the cloud dsitribution at different times at the same place, we still decide to combine the data into one to have more data for training. We also clear out the unlabeled data here because they are not helpful enough for classification problem. Since the data are not i.i.d., we cannot simply split the data by random. 

First method:
We decided to divide the data into 25 groups by cutting the image horizontally. In this case, data with y_coord from 2.0-78.2 and the corresponding x-coordinate are in the same group.  We randomly select 23 groups from these 25 groups as training/validation data and 2 groups as test data. In the 23 groups, we randomly sampled 18 groups as validation data. Splitting the data in this way looks like a one-stage cluster sampling. Every time we sample a group wihout replacement and include all data points in that group. Data in the same groups tend to be correlated to each other, which fit the property of the non i.i.d. data.

```{r}
set.seed(100)
all_clear = all[all$expert_label != 0, ]
min_y = min(all_clear$y_coord)
max_y = max(all_clear$y_coord)
det_y = (max_y - min_y) / 25
y_cut = seq(min_y, max_y + 2, det_y)
y_cut[length(y_cut)] = y_cut[length(y_cut)] + 1

train_val = sample(1:25, 23, replace = FALSE)
y_index = (train_val - 1) %% 25 + 1

train_val_data = all_clear
train_val_data$check = rep(FALSE, nrow(train_val_data))
for (i in 1:length(y_index)) {
  lower_y = y_cut[y_index[i]]
  upper_y = y_cut[y_index[i] + 1]
  train_val_data[train_val_data$y_coord >= lower_y & train_val_data$y_coord < upper_y, "check"] = TRUE
}

test_data = train_val_data[!train_val_data$check, ]
test_data = within(test_data, rm(check))
train_val_data = train_val_data[train_val_data$check, ]
train_val_data = within(train_val_data, rm(check))

y_train_index = sample(train_val, 18, replace = FALSE)
y_index = (y_train_index - 1) %/% 25 + 1
train_data = train_val_data
train_data$check = rep(FALSE, nrow(train_data))
for (i in 1:length(y_index)) {
  lower_y = y_cut[y_index[i]]
  upper_y = y_cut[y_index[i] + 1]
  train_data[train_data$y_coord >= lower_y & train_data$y_coord < upper_y, "check"] = TRUE
}
validation_data = train_data[!train_data$check, ]
train_data = train_data[train_data$check, ]
```


Our Second method is, for each image data set, to divide all the data into 25 groups by cutting the image into 5*5 small images. For instance, data with y_coord from 2.0-78.2 and x_coord from 65.0-125.8 are in the same group. We randomly select 23 groups from these 25 groups as training/validation data and 2 groups as test data. In the 23 groups, we randomly sampled 18 groups as validation data. Splitting the data in this way looks like stratified sampling. Every time we sample a group wihout replacement and include all data points in that group. We can also see this method as combining small pixels into a much larger one in a visual way. This method can guarantee pixels that have high correlations (adjacent pixels) can be sampled at the same time (except those on the boundary). And the random sampling on the 25 groups can make sure that we are not too sticking to the patterns for the training data so that overfitting can be somewhat prevented. 
```{r}
set.seed(100)
all_clear = all[all$expert_label != 0, ]
min_x = min(all_clear$x_coord)
max_x = max(all_clear$x_coord)
min_y = min(all_clear$y_coord)
max_y = max(all_clear$y_coord)
det_x = (max_x - min_x) / 5
det_y = (max_y - min_y) / 5
x_cut = seq(min_x, max_x + 2, det_x)
y_cut = seq(min_y, max_y + 2, det_y)
x_cut[length(x_cut)] = x_cut[length(x_cut)] + 1
y_cut[length(y_cut)] = y_cut[length(y_cut)] + 1

train_val = sample(1:25, 23, replace = FALSE)
x_index = (train_val - 1) %/% 5 + 1
y_index = (train_val - 1) %% 5 + 1
train_val_data = all_clear
train_val_data$check = rep(FALSE, nrow(train_val_data))
for (i in 1:length(x_index)) {
  lower_x = x_cut[x_index[i]]
  upper_x = x_cut[x_index[i] + 1]
  lower_y = y_cut[y_index[i]]
  upper_y = y_cut[y_index[i] + 1]
  train_val_data[train_val_data$x_coord >= lower_x & train_val_data$x_coord < upper_x & train_val_data$y_coord >= lower_y & train_val_data$y_coord < upper_y, "check"] = TRUE
}
test_data = train_val_data[!train_val_data$check, ]
test_data = within(test_data, rm(check))
train_val_data = train_val_data[train_val_data$check, ]
train_val_data = within(train_val_data, rm(check))

x_train_index = sample(train_val, 18, replace = FALSE)
x_index = (x_train_index - 1) %/% 5 + 1
y_index = (x_train_index - 1) %% 5 + 1
train_data = train_val_data
train_data$check = rep(FALSE, nrow(train_data))
for (i in 1:length(x_index)) {
  lower_x = x_cut[x_index[i]]
  upper_x = x_cut[x_index[i] + 1]
  lower_y = y_cut[y_index[i]]
  upper_y = y_cut[y_index[i] + 1]
  train_data[train_data$x_coord >= lower_x & train_data$x_coord < upper_x & train_data$y_coord >= lower_y & train_data$y_coord < upper_y, "check"] = TRUE
}
validation_data = train_data[!train_data$check, ]
train_data = train_data[train_data$check, ]
```



* 2(b)
```{r}
sum(test_data$expert_label == -1) / nrow(test_data)
sum(validation_data$expert_label == -1) / nrow(validation_data)
```
Setting all labels to -1, the accuracy for test data is 0.306 and that for validation data is 0.698, which are relatively low. If the test data and validation data contain mostly -1, the trivial classifier will have a high average accuracy.

* 2(c)
```{r}
library(ggbiplot)
library(corrplot)
image1_clear = image1[image1$expert_label != 0,]
pca_prcomp<- prcomp(image1_clear[,c("NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")], scale. = TRUE)
loadings = pca_prcomp$rotation
cor(image1_clear[,c("NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")], image1_clear$expert_label)
ggbiplot(pca_prcomp, choices = 1:2, scale = 0, alpha = 0) + ggtitle("biplot for image 1")
ggplot() + geom_point(aes(x = loadings[, 1], y=loadings[, 2])) +
  geom_text(aes(x = loadings[, 1], y=loadings[, 2], label=rownames(loadings))) +
  ggtitle("loadings for image 1")

x <- cor(image1_clear)
corrplot.mixed(x, lower.col = "black", number.cex = 0.6, title = "corrplot for image 1")
```



```{r}
image2_clear = image2[image2$expert_label != 0,]
pca_prcomp<- prcomp(image2_clear[,c("NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")], scale. = TRUE)
loadings = pca_prcomp$rotation
cor(image2_clear[,c("NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")], image2_clear$expert_label)
ggbiplot(pca_prcomp, choices = 1:2, scale = 0, alpha = 0) + ggtitle("biplot for image 2")
ggplot() + geom_point(aes(x = loadings[, 1], y=loadings[, 2])) +
  geom_text(aes(x = loadings[, 1], y=loadings[, 2], label=rownames(loadings))) +
  ggtitle("loadings for image 2")

x <- cor(image2_clear)
corrplot.mixed(x, lower.col = "black", number.cex = 0.6, title = "corrplot for image 2")
```

```{r}
image3_clear = image3[image3$expert_label != 0,]
pca_prcomp<- prcomp(image3[,c("NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")], scale. = TRUE)
loadings = pca_prcomp$rotation
cor(image3[,c("NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")], image3$expert_label)
ggbiplot(pca_prcomp, choices = 1:2, scale = 0, alpha = 0)+ ggtitle("biplot for image 3")
ggplot() + geom_point(aes(x = loadings[, 1], y=loadings[, 2])) +
  geom_text(aes(x = loadings[, 1], y=loadings[, 2], label=rownames(loadings))) +
  ggtitle("loadings for image 3")

x <- cor(image3_clear)
corrplot.mixed(x, lower.col = "black", number.cex = 0.7)
```

To find three best features, we first caculate the correlation between those features and the expert labels. The magnitude of the absolute values of the correlations represent how close the relationships are between features and expert labels. Larger correlation value means better features. We found that NDAI and CORR are better than others on average. 
Then, we plot biplots and loadings on first two PCs to see how much each feature contributes to these PCs. We choose to use two PCs because they capture almost 90% of the data which is enough to represent the entire data set. The loading plots can illustrate the association between features and PCs. The larger the loading of a feature in a given PC, the more associated the feature is with that PC. After viewing the four loading plots, we observe that SD and DF have larger loadings than other features (expect NDAI and CORR). We allow draw the correlation plot, 
Considering there are other reasons that is more associated with scientific explanation, we decide to be consistent with the three features chosen in the paper, which are CORR, NDAI and SD.

* 2(d)
```{r}
CVgeneric <- function(classifier, training_feature, training_label, K, loss_function, method) {
  bin = matrix(list(), nrow = 1, ncol = K)
  for (i in 1:23) {
    bin[[1, i %% K + 1]] = c(bin[[1, i %% K + 1]], i)
  }
  error = c()
  train_data = train_val_data
  train_data$check = rep(FALSE, nrow(train_data))
  for (i in 1:K) {
    for (j in bin[i]) {
      if (method == 2) {
        x_index = (j - 1) %/% 5 + 1
        y_index = (j - 1) %% 5 + 1
        for (k in 1:length(x_index)) {
          lower_x = x_cut[x_index[k]]
          upper_x = x_cut[x_index[k] + 1]
          lower_y = y_cut[y_index[k]]
          upper_y = y_cut[y_index[k] + 1]
          train_data[train_data$x_coord >= lower_x & train_data$x_coord < upper_x & train_data$y_coord >= lower_y & train_data$y_coord < upper_y, "check"] = TRUE
        }
       }else {
          y_index = j
          for (k in 1:length(y_index)) {
            lower_y = y_cut[y_index[k]]
            upper_y = y_cut[y_index[k] + 1]
            train_data[train_data$y_coord >= lower_y & train_data$y_coord < upper_y, "check"] = TRUE
          }
        }
     }
    CV_val = train_data[!train_data$check, ]
    CV_train = train_data[train_data$check, ]
    currFormula = as.formula(paste(training_label, "~", 
                                 paste(training_feature, collapse = "+"), sep = ""))
    if (identical(classifier, glm)) {
      model = classifier(currFormula, data = CV_train)
      y_prob = predict(model, CV_val, type = "response")
      y = CV_val[, training_label]
      y_hat = rep(-1, length(y_prob))
      y_hat[y_prob > 0.5] = 1
      error = c(error, loss_function(y_hat, y))
    }
    if (identical(classifier, lda)) {
      model = classifier(currFormula, data = CV_train)
      y_prob = predict(model, CV_val)
      y = CV_val[, training_label]
      y_hat = y_prob$class
      error = c(error, loss_function(y_hat, y))
    }
    if (identical(classifier, qda)) {
      model = classifier(currFormula, data = CV_train)
      y_prob = predict(model, CV_val)
      y = CV_val[, training_label]
      y_hat = y_prob$class
      error = c(error, loss_function(y_hat, y))
    }
    if (identical(classifier, knn)) {
      knn.pred = classifier(CV_train[, training_feature], CV_val[, training_feature], CV_train[, training_label], k = 3)
      error = c(error, loss_function(knn.pred, CV_val[, training_label]))
    }
    if (identical(classifier, svm)) {
      model = classifier(currFormula, data = CV_train, type = "C-classification", kernel = "linear", cost = 1)
      y_hat = predict(model, CV_val)
      y = CV_val[, training_label]
      error = c(error, loss_function(y_hat, y))
    }
  }
  return(error)
}
```

## Problem 3
3(a)
```{r}
library(MASS)
library(class)
library(e1071)
accuracy <- function(y_hat, y) {
  return(mean(y_hat == y))
}
glm_accuracy = CVgeneric(glm, c("NDAI", "SD", "CORR"), c("expert_label"), 8, accuracy, method = 1)
glm_accuracy_avg = mean(glm_accuracy)
model_glm = glm(expert_label ~ NDAI + SD + CORR, data = train_val_data)
y_prob_glm = predict(model_glm, test_data, type = "response")
y = test_data[, "expert_label"]
y_hat_glm = rep(-1, length(y_prob_glm))
y_hat_glm[y_prob_glm > 0.5] = 1
test_accuracy_glm = accuracy(y_hat_glm, y)


lda_accuracy = CVgeneric(lda, c("NDAI", "SD", "CORR"), c("expert_label"), 8, accuracy, method = 1)
lda_accuracy_avg = mean(lda_accuracy)
model_lda = lda(expert_label ~ NDAI + SD + CORR, data = train_val_data)
y_prob_lda = predict(model_lda, test_data, type = "response")
y = test_data[, "expert_label"]
y_hat_lda = y_prob_lda$class
test_accuracy_lda = accuracy(y_hat_lda, y)

qda_accuracy = CVgeneric(qda, c("NDAI", "SD", "CORR"), c("expert_label"), 8, accuracy, method = 1)
qda_accuracy_avg = mean(qda_accuracy)
model_qda = qda(expert_label ~ NDAI + SD + CORR, data = train_val_data)
y_prob_qda = predict(model_qda, test_data, type = "response")
y = test_data[, "expert_label"]
y_hat_qda = y_prob_qda$class
test_accuracy_qda = accuracy(y_hat_qda, y)

knn_accuracy = CVgeneric(knn, c("NDAI", "SD", "CORR"), c("expert_label"), 8, accuracy, method = 1)
knn_accuracy_avg = mean(knn_accuracy)
knn.pred = knn(train_val_data[, c("NDAI", "SD", "CORR")], test_data[, c("NDAI", "SD", "CORR")], train_val_data[, "expert_label"], k = 3)
test_accuracy_knn = accuracy(knn.pred, test_data[, "expert_label"])

svm_accuracy = CVgeneric(svm, c("NDAI", "SD", "CORR"), c("expert_label"), 8, accuracy, method = 1)
svm_accuracy_avg = mean(svm_accuracy)
model_svm = svm(expert_label ~ NDAI + SD + CORR, data = train_val_data, type = "C-classification", kernel = "linear", cost = 1)
y_hat_svm = predict(model_svm, test_data)
y = test_data[, "expert_label"]
test_accuracy_svm = accuracy(y_hat_svm, y)

model_accuracy = data.frame(glm = glm_accuracy, lda = lda_accuracy, qda = qda_accuracy, knn = knn_accuracy, svm = svm_accuracy)
model_accuracy = rbind(model_accuracy, c(glm_accuracy_avg, lda_accuracy_avg, qda_accuracy_avg, knn_accuracy_avg, svm_accuracy_avg))
rownames(model_accuracy) = c(seq(1, 8), "average")
model_accuracy

test_accuracy = data.frame(glm = test_accuracy_glm, lda = test_accuracy_lda, qda = test_accuracy_qda, knn = test_accuracy_knn, svm = test_accuracy_svm)
test_accuracy
```


3(b)
```{r}
library(ROCR)
library(pROC)
rocplot <- function(pred, truth, main) {
  predob = prediction(pred, truth)
  perf = performance(predob, "tpr", "fpr")
  return(perf)
}
bestalpha <- function(x, y, alpha) {
  minsqrdist = 1000000
  index = 0
  for (i in 1:length(x)) {
    if ((x[i] ^ 2 + (y[i] - 1) ^ 2) < minsqrdist) {
      minsqrdist = x[i] ^ 2 + (y[i] - 1) ^ 2
      index = i
    }
  }
  return(index)
}


glmfit.opt = glm(expert_label ~ NDAI + SD + CORR, data = train_val_data)
fitted_glm = predict(glmfit.opt, test_data)
perf_glm = rocplot(fitted_glm, test_data[, "expert_label"], "ROC of GLM test")
x_glm = slot(perf_glm, 'x.values')[[1]]
y_glm = slot(perf_glm, 'y.values')[[1]]
alpha_glm = slot(perf_glm, "alpha.values")[[1]]
index_glm = bestalpha(x_glm, y_glm, alpha_glm)
glm = data.frame(x_axis = x_glm, y_axis = y_glm, label = rep("GLM", length(x_glm)))
ggplot(glm, aes(x = x_axis, y = y_axis)) + geom_line() + geom_point(aes(x = x_glm[index_glm], y = y_glm[index_glm], color = "red"), show.legend = FALSE) + annotate("text", x = x_glm[index_glm], y = y_glm[index_glm] + 0.08, label = "alpha = -0.2624315" ) + ggtitle("ROC for GLM") + xlab("False Positive Rate") + ylab("True Positive Rate")
roc(test_data[, "expert_label"], fitted_glm, plot = TRUE, print.auc = TRUE)
x_glm[index_glm] ^ 2 + (y_glm[index_glm] - 1) ^ 2


svmfit.opt = svm(expert_label ~ NDAI + SD + CORR, data = train_val_data, kernel = "linear", cost = 1)
fitted_svm = attributes(predict(svmfit.opt, test_data, decision.values = TRUE))$decision.values
perf_svm = rocplot(fitted_svm, test_data[, "expert_label"], "ROC of SVM test")
x_svm = slot(perf_svm, 'x.values')[[1]]
y_svm = slot(perf_svm, 'y.values')[[1]]
alpha_svm = slot(perf_svm, "alpha.values")[[1]]
index_svm = bestalpha(x_svm, y_svm, alpha_svm)
svm = data.frame(x_axis = x_svm, y_axis = y_svm, label = rep("SVM", length(x_svm)))
ggplot(svm, aes(x = x_axis, y = y_axis)) + geom_line() + geom_point(aes(x = x_svm[index_svm], y = y_svm[index_svm], color = "red"), show.legend = FALSE) + annotate("text", x = x_svm[index_svm], y = y_svm[index_svm] + 0.08, label = "alpha = 0.01198434" ) + ggtitle("ROC for SVM") + xlab("False Positive Rate") + ylab("True Positive Rate")
x_svm[index_svm] ^ 2 + (y_svm[index_svm] - 1) ^ 2
roc(test_data[, "expert_label"], fitted_svm, plot = TRUE, print.auc = TRUE)



ldafit.opt = lda(expert_label ~ NDAI + SD + CORR, data = train_val_data)
fitted_lda = predict(ldafit.opt, test_data)
perf_lda = rocplot(fitted_lda$posterior[,2], test_data[, "expert_label"], "ROC of LDA test")
x_lda = slot(perf_lda, 'x.values')[[1]]
y_lda = slot(perf_lda, 'y.values')[[1]]
alpha_lda = slot(perf_lda, "alpha.values")[[1]]
index_lda = bestalpha(x_lda, y_lda, alpha_lda)
lda = data.frame(x_axis = x_lda, y_axis = y_lda, label = rep("LDA", length(x_lda)))
ggplot(lda, aes(x = x_axis, y = y_axis)) + geom_line() + geom_point(aes(x = x_lda[index_lda], y = y_lda[index_lda], color = "red"), show.legend = FALSE) + annotate("text", x = x_lda[index_lda], y = y_lda[index_lda] + 0.08, label = "alpha = 0.1961443" ) + ggtitle("ROC for LDA") + xlab("False Positive Rate") + ylab("True Positive Rate")
x_lda[index_lda] ^ 2 + (y_lda[index_lda] - 1) ^ 2
roc(test_data[, "expert_label"], fitted_lda$posterior[,2], plot = TRUE, print.auc = TRUE)

qdafit.opt = qda(expert_label ~ NDAI + SD + CORR, data = train_val_data)
fitted_qda = predict(qdafit.opt, test_data)
perf_qda = rocplot(fitted_qda$posterior[,2], test_data[, "expert_label"], "ROC of QDA test")
x_qda = slot(perf_qda, 'x.values')[[1]]
y_qda = slot(perf_qda, 'y.values')[[1]]
alpha_qda = slot(perf_qda, "alpha.values")[[1]]
index_qda = bestalpha(x_qda, y_qda, alpha_qda)
qda = data.frame(x_axis = x_qda, y_axis = y_qda, label = rep("QDA", length(x_qda)))
ggplot(qda, aes(x = x_axis, y = y_axis)) + geom_line() + geom_point(aes(x = x_qda[index_qda], y = y_qda[index_qda], color = "red"), show.legend = FALSE) + annotate("text", x = x_qda[index_qda], y = y_qda[index_qda] + 0.08, label = "alpha = 0.04849714" ) + ggtitle("ROC for QDA") + xlab("False Positive Rate") + ylab("True Positive Rate")
x_qda[index_qda] ^ 2 + (y_qda[index_qda] - 1) ^ 2
roc(test_data[, "expert_label"], fitted_qda$posterior[,2], plot = TRUE, print.auc = TRUE)

```


## Problem 4
After comparing two splitting methods and different classification methods, we believe that, in our case, the second splitting method is better than the first one, and QDA has best performance on this splitted data.

* 4(a)
```{r}
train_cloud = train_val_data[train_val_data$expert_label == 1, c("NDAI", "SD", "CORR")]
colMeans(train_cloud)
cov(train_cloud)
train_empty = train_val_data[train_val_data$expert_label == -1, c("NDAI", "SD", "CORR")]
colMeans(train_cloud)
cov(train_empty)
```
The means and covariances of each class are shown as follows:

The high variances (covariances between features themselves) means that the corresponding features have a lot of expressiveness. In other words, features with low variances are close to a constant. In our case, we prefer features with larger variances. For the rest covariances (covariances between different features), features with high covariances have a lot of redundancy for each other. Since we hope the features can capture as much as possible, we prefer features with low covariances.

In our covariance matrices, SD has very high variance and relatively low covariance. SD is the best feature of these three. CORR has lowest variance and relatively high covariance. We may want to improve our model by changing this feature to another.



