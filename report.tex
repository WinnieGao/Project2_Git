\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,float,amsfonts,enumitem,tcolorbox}
\usepackage{enumitem}
\usepackage[margin=0.6in]{geometry}

\usepackage{graphicx}
\usepackage{url}


\title{Project 2 Cloud Data}
\author {Caiyi Deng 3033303379, Winnie Gao 3031884025}


\usepackage{Sweave}
\begin{document}
\input{report-concordance}

\maketitle

\section{Data Collection and Exploration}

\vspace{0.2cm}
\textbf{a. Summary of Paper}

Climate changes has been a popular topic in scientific research. Particularly, the warming Arctic is one of the biggest stories in our times. Global climate models predict that the increasing atmospheric carbon dioxide levels is strongly related to the increasing surface air temperatures in the Arctic, where cloud plays an important role in producing more atmospheric carbon dioxide. In this paper, scientists use the Multiangle Imaging SpectroRadiometer (MISR) imagery to perform a cloud detection to ascertain whether cloud can potentially lead to further warming in the Arctic.

MISR collects a massive amount of data from its nine cameras viewing at a different angel in four spectral. It covers the daylight side of the Earth from the Arctic down to Antarctica in 45 minutes and completes all paths in 16 days of a cycle. Each path is subdivided into blocks, with the block numbers increasing from the North Pole to South Pole, and each complete trip of MISR around the Earth is counted as a unique orbit. However, due to the transmission channel constraints, only the red radiances and all channels from the nadir camera are transmitted at full 275m * 275m resolution. The remaining blue, green and near-infrared radiances from the non-nadir cameras are aggregated to a lower resolution before transmission. 

Scientists utilizes correlations in brightness among multiple MISR views of the same scene under cloud-free conditions to model the surface. This new algorithm, enhanced linear correlation matching (ELCM), is based on thresholding three features: the correlation (CORR) of MISR images, the standard deviation (SD) of the MISR nadir camera pixel values, and a normalized difference angular index (NDAI) to create labels for classification. Then, the resulting labels are used in the second algorithm, ELCM-QDA, to produce more informative probability prediction. 

The results suggest that the ELCM algorithm based on the three features outperforms those existing algorithms based only on the radiation measurement, where it provides better spatial coverage for cloud detection in the Arctic. Moreover, the ELCM algorithm combines classification and clustering framework to fit the MISR data processing, which helps improve the computational speed online effectively. 

This research not only creates a better algorithm to implement cloud detection but also encourages further study on the changing cloud properties to the warming Arctic. In addition, it demonstrates the significant impact of statistics in successfully solving a modern scientific problem. Statisticians are now directly involved in the data processing and use powerful statistical thinking to help tackle challenges. \par

\vspace{0.3cm}
\mbox{}\\
\textbf{b. Summery of Data}

In image1 data set, there are 115229 data points. We calculate the percentage for pixels in two different classes: 17.77\% pixles are classified as cloud, 43.78\% pixels are classified as not cloud and 38.46\% pixels are marked as unlabeled. In image2 data set, there are 115110 data points. 34.11\% pixels are classified as cloud, 37.25\% pixels are classified as not cloud and 28.64\% pixels are marked as unlabeled. In image3 data set, there are 115217 data points. 18.44\% classified as cloud, 29.29\% as not cloud and 52.27\% as unlabeled. After we combined three image data sets, there are 345556 data points in total: 23.43\% classified as cloud, 36.78\% as not cloud and 39.79\% as unlabeled. 

In order to view the pattern or trend for the data points, we plot scatter plots and color each data point according to their labels. 


\includegraphics[width = 6cm]{1(b)image1.png}
\includegraphics[width = 6cm]{1(b)image2.png}
\includegraphics[width = 6cm]{1(b)image3.png}

From the three plots, we can see the patterns that the pixels with same labels are more tend to connect to each other, and unmarked pixels stay around those marked as clouds. This pattern contradicts to the assumption in the paper that the samples are independent and identically distributed because the pixels adjcent to pixels marked as cloud are more likely to be marked as cloud.


\vspace{0.3cm}
\mbox{}\\
\textbf{c. EDA}

To further explore our data, we also summarize the pairwise relationship between the features themselves.

\includegraphics[width = 6cm]{1(c)image1.png}
\includegraphics[width = 6cm]{1(c)image2.png}
\includegraphics[width = 6cm]{1(c)image3.png}

The three pairwise scatterplots all show that DF, CF, BF, AF and AN shows a linear relationship between each other, especially for AF and AN, AF and BF. However, we observe that data points in the third pairwise scatterplot spread more widely than in the first two pairwise sctterplots. The differences between three graphs can also show that the (linear) relationships decrease over time.

After checking pairwise relationship plots, we study the relationship between each independent featues and their expert labels for each image separately and together.

\mbox{}\\
(relationship between expert label and NDAI)

\includegraphics[width = 4.5cm]{1(c)image4.png}
\includegraphics[width = 4.5cm]{1(c)image5.png}
\includegraphics[width = 4.5cm]{1(c)image6.png}
\includegraphics[width = 4.5cm]{1(c)image7.png}

The boxplots for CORR based on different labels show that CORR is on average higher for pixels marked as cloud than as not cloud. CORR also has a higher variance for those marked as cloud.

This conclusion contradicts to the statement in the paper that high correlations over cloud-free or low-cloud areas are expected. This is because high correlations also occur under rare circumstances due to cloud movement. More importantly, recklessly declaring clear for high CORR pixels and cloudy for low CORR pixels will produce errors because of the smoothness of surface terrain and the difference of attitudes of clouds.

Therefore, we should also involve the feature SD to identify surfaces into our investigation. We first plot SD vs. Label independently for each image and all data.

\mbox{}\\
(relationship between expert label and SD)

\includegraphics[width = 4.5cm]{1(c)image8.png}
\includegraphics[width = 4.5cm]{1(c)image9.png}
\includegraphics[width = 4.5cm]{1(c)image10.png}
\includegraphics[width = 4.5cm]{1(c)image11.png}

From the boxplots for SD based on different labels, we can clearly see that the SD is higher for cloud pixels than those cloud-free ones. But the cloud-free pixels spread more widely. It can be explained that SD are usually small for radiation emanating from smooth surfaces.

Finally, the thrid feature NDAI relates to the differences for isoreopic level of surface-leaving radiation between low-attitude clouds and snow-coved surfaces.

\mbox{}\\
(relationship between expert label and NDAI)

\includegraphics[width = 4.5cm]{1(c)image12.png}
\includegraphics[width = 4.5cm]{1(c)image13.png}
\includegraphics[width = 4.5cm]{1(c)image14.png}
\includegraphics[width = 4.5cm]{1(c)image15.png}

From the boxplots for NDAI based on different labels, we can clearly see that the NDAI is higher for cloud pixels than those cloud-free ones. The distrubition for cloud-free pixels are left skewed, while the distribution for pixels marked as cloud are roughly symmetric. The distribution for three different image data are roughly the same as well.

\section{Preparation}

\vspace{0.2cm}
\textbf{a. Data Split}

Even though the three image data sets represent the cloud dsitributions at different times at the same place, we decide to merge all data into one to have more data for training. We also clear out the unlabeled data here because they are not helpful for classification. Since the data are not i.i.d., we cannot simply split the data by random. To solve this problem, we come up with two different ways to split data into training, validation and test set.

First method:
We divide the data into 25 groups by cutting the image horizontally. For example, data with y\_coord from 2.0 to 78.2 are in the same group.  We randomly select 23 groups from these 25 groups as training/validation data and 2 groups as test data. In the 23 groups, we randomly sampled 18 groups as validation data. Splitting data in this way looks like a one-stage cluster sampling. Every time we sample a group wihout replacement and include all data points in that group. Data in the same groups tend to be correlated to each other, which fits the property of the non-i.i.d. data.

Our Second method is, for each image data set, to divide all the data into 25 groups by cutting the image into 5*5 sub-images. For instance, data with y\_coord from 2.0-78.2 and x\_coord from 65.0-125.8 are in the same group. Same as the first method, we randomly select 23 groups from these 25 groups as training/validation data and 2 groups as test data. In the 23 groups, we randomly sampled 18 groups as validation data. Splitting the data in this way looks like stratified sampling. Every time we sample a group wihout replacement and include all data points in that group. We can also see this method as combining small pixels into a much larger one in a visual way. This method can guarantee pixels that have high correlations (adjacent pixels) can be sampled at the same time (except those on the boundary). And the random sampling on the 25 groups can make sure that we are not too sticking to the patterns for the training data so that overfitting can be somewhat prevented. 

\vspace{0.3cm}
\mbox{}\\
\textbf{b. Baseline}

Setting all labels to -1, the accuracy for test data is 0.520 and that for validation data is 0.270, both of which are very low. Only if pixels marked as -1 have high frequency in the validation and test set, the trivial classifier will have a high average accuracy.

\vspace{0.3cm}
\mbox{}\\
\textbf{c. First Order Importance}

Recall in the EDA part, we notice that five raw features(DF, CF, BF, AF, AN) have high linear relationship between each other. To have more information represented by features, we will choose at most one from these five features.

To find three best features, we first caculate and plot the correlation between each feature and expert label. The magnitudes of the absolute values of the correlations represent how close the relationships are between features and expert labels. Larger correlation value means a better feature. We find that NDAI and CORR are better than others on average. 

\includegraphics[width = 5.5cm]{2(c)image1.png}
\includegraphics[width = 5.5cm]{2(c)image2.png}
\includegraphics[width = 5.5cm]{2(c)image3.png}

Then, we plot biplots and loadings on first two PCs to see how much each feature contributes to these PCs. We choose to use two PCs because they capture almost 90\% of the data which is enough to represent the entire data set. The loading plots can illustrate the association between features and PCs. The larger the loading of a feature is in a given PC, the more association the feature contains with that PC. After viewing the four loading plots, we observe that SD and DF have larger loadings than other features (expect NDAI and CORR). Considering there are other reasons that is more associated with scientific explanation, we decide to be consistent with the three features chosen in the paper, which are CORR, NDAI and SD.


\includegraphics[width = 13cm]{2(c)pca1.png}

\includegraphics[width = 13cm]{2(c)pca2.png}

\includegraphics[width = 13cm]{2(c)pca3.png}

\vspace{0.3cm}
\mbox{}\\
\textbf{d. CVgeneric Function}

See \url{https://github.com/WinnieGao/Project2_Git} for more detailed explanation and code.



\section{Modeling}

\vspace{0.2cm}
\textbf{a. Different Classification Models}

To train the data, we try several different classification methods: Generalized Linear Model, Linear Discriminant Analysis, Quadratic Discriminant Analysis, K-nearest Neighbor and Support Vector Machine.
For GLM, LDA and QDA, since, in our natural world, most phenomena can be approximated using Gaussion distribution, we assume our data sets are under Gaussian distribution, which means LDA and QDA are feasible. Since Gaussian distribution is exponential family, we can also apply GLM in our case. And there is no specific assumptions for using KNN and SVM.

We use cross-validation with 8 folds to train the models, and get accuracies across all folds and average them. We also apply the model on the test set and get an accuracy on the test set.

For our first splitting method, the cross-validataion averaged accuracies show that LDA, QDA,SVM and KNN all return great performance with an accuracy above 0.91. Among them, LDA performs the best on the validataion data with 0.9286555 accauray and GLM gives the lowest average cross-validation accuarcy. As for the performance on the test set, all models gives lower accurary about 0.7 to 0.8. KNN gives the best accuracy and GLM returns the worst performance on the test set. Considering both the accuracies on the validation and test set, QDA performs relatively better than other models. Hence, we conclude that QDA is the best model for the first spliting data.  

\includegraphics[width = 14.5cm]{3(a)1acuracy}

For the second one, the cross-validation average accuracies demonstrate that LDA have the best performance on validation data. QDA also has a high accuracy on test data, but LDA performs better. GLM has lowest average cross-validation accuracy on validation set and svm performs worst on test data. Considering both average validation set accuracies and test set accuracies, we believe that QDA is also the best model for the second splitting method.

\includegraphics[width = 15.5cm]{3(a)2acuracy}

\vspace{0.3cm}
\mbox{}\\
\textbf{b. ROC curves}

We then plot ROC curves for each classification method. We don't plot a ROC curve for KNN, because the model does not calculate the probabily of being each class. It directly gives the classification result instead, and we don't really think it is worthy changing the hyperparameter K here. 

In each model, we choose the cutoff value that has the smallest distance to (0, 1). We do so because we want to maximize true positive rate and minimize false positive rate at the same time. We did not put more weight on true positive rate because, unlike the medical cases, the incorrect classifications of both clouded or cloud-free pixels have same consequence on our result.

To choose a best method according to the ROC curves, we calculate the distance between the point we marked and (0, 1). The smaller distance indicates a better model.

\mbox{}\\
The ROC curves are drawn below with cutoff value we choose marked for first splitting method:

\includegraphics[width = 7.5cm]{3(b)1-1}
\includegraphics[width = 7.5cm]{3(b)1-2}

\includegraphics[width = 7.5cm]{3(b)1-3}
\includegraphics[width = 7.5cm]{3(b)1-4}

In the first spliting method, GLM and LDA both have the shortest distance 0.1291 to the point (0,1) and the same value 0.8239 for the area under the ROC curve value.SVM returns a slightly larger distance 0.1296 but covers the least area under the ROC curve with value 0.7854. QDA gives the largest distance 0.1637 to the point (0,1) and the largest AUC value 0.8391. Noticed that the ROC curves for four models are not smooth and some curves are slightly convex, which are different from usual ROC curve. This is because the cloud data set cannot be separated with a linear combination of featues. All our models assume that the data are linear separable, which might result in the abnormal shape of the RUC cruve.  

Similarly, we analyze the the ROC curves for second spliting method:

\includegraphics[width = 7.5cm]{3(b)2-1}
\includegraphics[width = 7.5cm]{3(b)2-2}

\includegraphics[width = 7.5cm]{3(b)2-3}
\includegraphics[width = 7.5cm]{3(b)2-4}

In the second spliting method, GLM has a distance of 0.019, SVM has a distance of 0.021, LDA has a distance of 0.019, QDA has a distance of 0.023. The distances of different methods does not change very much. Then, we also calculate the AUC for each classification method. The larger AUC indicate a better model. The area under GLM's ROC curve is 0.9635, the area under SVM's ROC curve is 0.9613, the area under LDA's ROC curve is 0.9635 and the area under QDA's ROC curve is 0.9581. Considering both of these two factors, for the second splitting method, we think GLM and LDA are better than the rest of two.


\vspace{0.3cm}
\mbox{}\\
\textbf{c. (Bonus) Assessing with New Metric}

Having a high accuracy is not enough to evaluate a particular model. Suppose we have a trivial classifier, which is predicting all data points as 1, we may still get a very high accuracy since most data points are marked as 1 in our training and test data set, but it by no means proves this model is good.

Therefore, we come up with two new metrics that can avoid this kind of coincidence: Among all pixels labeled as cloud, what is the proportion of the pixels that have true label as cloud. Simiarly, Among all pixels with true label as cloud, what is the proportion of pixels we predicted as cloud. The first is called "precision" and the second one is called "recall". Since they defined very similarly and have similar effect, in this report, we only use precision as a new metric to evaluate our model to save time.

The precision for our models are as follows:

\includegraphics[width = 12.5cm]{3(c)1}

We can see that test precisions of our models give different ranking from our test accuracies.


\section{Diagnostics}

\vspace{0.2cm}
\textbf{a. In-depth Analysis}

According to the analysis above, we think second splitting method is better and QDA is the best classification method in our case. Therefore, we do some in-depth analysis on QDA for the second splitting method.

The means and covariances of each class are shown as follows:

\mbox{}\\
Mean and Covariance matrix for design matrix classified as cloud

\includegraphics[width = 7.5cm]{4(a)1}
\includegraphics[width = 7.5cm]{4(a)2}

\mbox{}\\
Mean and Covariance matrix for design matrix classified as cloud-free

\includegraphics[width = 7.5cm]{4(a)3}
\includegraphics[width = 7.5cm]{4(a)4}

The high variances (covariances between features themselves) means that the corresponding features have a lot of expressiveness. In other words, features with low variances are close to a constant. In our case, we prefer features with larger variances. For the rest covariances (covariances between different features), features with high covariances have a lot of redundancy for each other. Since we hope the features can capture as much as possible, we prefer features with low covariances.

In our covariance matrices, SD has very high variance and relatively low covariance. SD is the best feature of these three. CORR has lowest variance and relatively high covariance. We may want to improve our model by changing this feature to another.

\vspace{0.3cm}
\mbox{}\\
\textbf{b. Misclassification Analysis}

For our best classification model which is QDA, we analyze the misclassification by showing ranges of feature values. We also apply our trained model to three different image data sets and plot the misclassified data point on the original map to check if they are in particular regrions.

In the test data, we pick all misclassified data points and plot density curves for each feature based on two different classification errors (False Positive and False Negative). And we compare these curves with that of data with true label.

\includegraphics[width = 7.5cm]{4(b)NDAI1}
\includegraphics[width = 7.5cm]{4(b)NDAI2}

\includegraphics[width = 7.5cm]{4(b)SD1}
\includegraphics[width = 7.5cm]{4(b)SD2}

\includegraphics[width = 7.5cm]{4(b)CORR1}
\includegraphics[width = 7.5cm]{4(b)CORR2}

The plots on the left are data marked as cloud-free (both correctly and incorrectly classified) and the plots on the right are those marked as clouded. In the first plot(Density Curve for NDAI: Cloud-free vs. FP), two plots have different distribution: the curve for true labeled data is right-skewed and the curve for false labeled data is left skewed, which demonstrates that, when NDAIs of the pixels have high values, our classifier will easily predict them as clouded although they are cloud-free. For data that are misclassified as cloud-free, NDAI has a very narrow range: from around 1.35 to 1.6. Data with NDAI in this range are very likely to be misclassified as cloud-free. Density curves of SD on the left shows that data with higher SD have larger possibility to be misclassified as cloud. For data that are misclassified as cloud-free, SD has a extremely small range compared with true label: from 0 to about 3. Most data with SD in this range are misclassified as cloud-free. CORR does not have particular pattern for data misclassified as cloud, but for those misclassified as cloud-free, CORR has a small range: from about -0.15 to 0.15. Similarly, data with CORR in this range have very high possibility to be classified as cloud-free.

Then, we use our trained QDA model to predict data points in three image data sets (data points with expert label of 0 are excluded). And plot the predicted labels for each image data set.

\includegraphics[width = 6cm]{4(b)7}
\includegraphics[width = 6cm]{4(b)8}
\includegraphics[width = 6cm]{4(b)9}

Comparing these three scatterplots with those in Part1b, we notice that there are several inconsistencies in our plots. The inconsistencies imply the misclassified data points. 

We notice that the data at the boundary of clouded and cloud-free pixels are often misclassified. More importantly, we realize that there are several small blocks that are entirely misclassified. For example, in the first image, the small patch between the upper left two larger patches are classified as cloud while they should have been marked as cloud-free. This is probably because our model is not expressive enough to sperate out the rapid change. In image2, we have significantly fewer misclassified data points. This again verifies that our model is underfitting for image1. The same explanation can also be applied to the third image.


\vspace{0.3cm}
\mbox{}\\
\textbf{c. Better Classifier}

Based on the analysis in the previous two sections, we come up with two ways to improve our classifier.
First, as we mentioned above, the three features selected are not expressive enough to train our model. We decide to add a new feature to train our QDA. Recall the PCA figures we plotted in the second part. Among the rest of the five raw features, DF stands out as the best. Therefore, we add DF as a new feature and train our QDA classification model again on our training data. The final accuracy on test data arises from 0.9155 to 0.9251, which is better than the original QDA model. For future data without expert label, we believe our data can work well because our average accuracy over 8 folds cross validation and test accuracy are quite high. Moreover, QDA won't take that much time to train and predict as other classification methods like svm and knn.

Secondly, QDA requires that the data should be Gaussian distribution, but we cannot ensure that the future data are strictly meet this requirement. Therefore, we also tried Random Forest to train our data. Since we don't have a large number of features, Random Forest is suitable in our case.

\includegraphics[width = 10.5cm]{4(c)RandomForest}

We choose to consider 3 features at each split and 25 trees for each forest. We train our model on train data and get a test accuracy of 0.9224 on test data, which perform better than QDA model with test accuary 0.9155. Moreover,random forest does not have particular assumption on the data set, the accuracy on future data should remain around the same. Hence, we confirm that random forest is a better classifier. 

\vspace{0.3cm}
\mbox{}\\
\textbf{d. Changes after Modification of Splitting Method}

The results in parts 4(a) and 4(b) change as we use splitting method 1 using QDA model.

\mbox{}\\
Mean and Covariance matrix for design matrix classified as cloud using splitting method 1

\includegraphics[width = 7.5cm]{4(d)1}
\includegraphics[width = 7.5cm]{4(d)2}

\mbox{}\\
Mean and Covariance matrix for design matrix classified as cloud-free using splitting method 1

\includegraphics[width = 7.5cm]{4(d)3}
\includegraphics[width = 7.5cm]{4(d)4}

Applying the same feature preference rule we use in 4(a), a good feature is the one with larger variances and low covariances. Among three features, SD is still the best features since it has the largest variance and lowest covariance.

\includegraphics[width = 7.5cm]{4(d)ndai1}
\includegraphics[width = 7.5cm]{4(d)ndai2}

\includegraphics[width = 7.5cm]{4(d)sd1}
\includegraphics[width = 7.5cm]{4(d)sd2}

\includegraphics[width = 7.5cm]{4(d)corr1}
\includegraphics[width = 7.5cm]{4(d)corr2}

Similarly, we draw density plots to analyze any misclassification as we use splitting method 1. For NDAI, the cloud-free true labeled density plot is left-skewed and false positive density plot is right skewed, which means that the model tends to misclassify not-cloud data when the value of NDAI is large. While the false negative for cloud in NDAI has an approximately normal distribution, indicating that there is no specific pattern for misclassifying cloud as not-cloud. Considering the feature SD, the false positive density also indicate no particular pattern for misclassification but there are relatively large amount of could-free data are misclassified as cloud. Additionally, the false negative in SD feature shows that the model would classify could data as cloud-free when SD  is  small. For CORR, no special patterns can be found in  both cloud and could-free density plots, but many data are misclassified when only considering CORR. 

\includegraphics[width = 6cm]{4(d)image1}
\includegraphics[width = 6cm]{4(d)image2}
\includegraphics[width = 6cm]{4(d)image3}

Comparing the scatterplots in 4(b), we mark the difference among these graphs. In general, using splitting method 1 gives similar scatterplots, which indicates that different ways of splitting data have little impact on the QDA model. If we split the data differently, the distribution in each data group would be different. However, the model still work well and give similar classification. This agrees with the results in 3(b), where the test accuracy in QDA for the two splitting methods are relatively higher than other models. Hence, we confirm that future data will work well in QDA model. 

\vspace{0.3cm}
\mbox{}\\
\textbf{e. Conclusion}

Considering the cloud data are not independent and identically distributed, we come up with two splitting methods to train classification models. We implement GLM, LDA, QDA, KNN and SVM models on train and test data with different splitting methods. The results from two approaches show that QDA is the best model among them. To seak improvement on the classifier, we add a new feature to the model. Also, we run random forest and check the model performance. Without requirement of Gaussian distribution on the data, random forest gives better accuracy than QDA. Finally, we analyze misclassification in term of distinct data splitting approaches. It turns out that different splitting methods only slightly change the classification results, which means QDA will have stable performance on future data with different
distributions. 

\vspace{0.3cm}
\mbox{}\\
\textbf{Acknowlegement}

As the project requires two data splitting methods, so each of us is responsible for one method. Winnie worte all the code and explaintions for method 2 and Caiyi are in charge of all parts for method 1. We meet almost every day to exchange our opinion on the project. R documentation and ISLR text book are our resources while we need help on model concept and implementation. 

Github Link: \url{https://github.com/WinnieGao/Project2_Git}


\end{document}
